---
title: "ENMeval v1.0.0 Vignette"
author: "Jamie M. Kass, Robert Muscarella, and Peter Galante"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ENMeval v1.0.0 Vignette}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, echo = FALSE, include=FALSE}
library(knitr)
knitr::opts_chunk$set(collapse=TRUE, message=FALSE, warning=FALSE, eval=FALSE, comment="#>")
```

- [Introduction](#intro)
- [Data Acquisition & Pre-processing](#data)
- [Partitioning Occurrences for Evaluation](#partition)
- [Running ENMeval](#eval)
- [Plotting results](#plot)
- [Downstream Analyses](#downstream)
- [Resources](#resources)


## Introduction {#intro}

[`ENMeval`](https://cran.r-project.org/package=ENMeval) is an R package that performs automated runs and evaluations of ecological niche models (ENMs, a.k.a. SDMs), which can estimate species' ranges and niche characteristics from data on species occurrences and environmental variables. 

Some of the most frequently used ENMs are machine learning algorithms with settings that can be "tuned" to determine optimal levels of model complexity. In implementation, this means building models of varying settings, then evaluating them and comparing their performance to select the optimal settings. Such tuning exercises can result in models that balance goodness-of-fit (i.e., avoiding overfitting) and predictive ability. Model evaluation is often done with cross validation, which consists of partitioning the data into groups, building a model with all the groups but one, evaluating this model on the left-out group, then repeating the process until all groups have been left out once.

The primary function, `ENMevaluate`, does all the heavy lifting and returns several items including a table of evaluation statistics and, for each setting combination (or *run*), a model object and a raster layer showing the model prediction across the study extent. There are also options for calculating niche overlap between predictions, running in parallel to speed up computation, and more. For a more detailed description of the package, check out the open-access publication:

[Muscarella, R., Galante, P. J., Soley-Guardia, M., Boria, R. A., Kass, J. M., Uriarte, M. and Anderson, R. P. (2014), ENMeval: An R package for conducting spatially independent evaluations and estimating optimal model complexity for Maxent ecological niche models. Methods in Ecology and Evolution, 5: 1198–1205.](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12261/full)

Older versions (v0.3.0 and earlier) implemented only [Maxent](http://biodiversityinformatics.amnh.org/open_source/maxent/) and [maxnet](https://cran.r-project.org/package=maxnet), but v2.0.0 and onward can implement any ENM in theory provided its settings are specified in a ENMdetails object (see below for an example). This version has additional built-in algorithms: [boosted regression trees](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.1365-2656.2008.01390.x) (boostedRegressionTreess), [BIOCLIM](https://onlinelibrary.wiley.com/doi/full/10.1111/ddi.12144), , but users can provide their own custom model specifications. 

## Data Acquisition & Pre-processing {#data}
In this vignette, we briefly demonstrate acquisition and pre-processing of input data for `ENMeval`. There are a number of other excellent tutorials on these steps, some of which we compiled in the [Resources](#resources) section.

We'll start by downloading an occurrence dataset for [*Bradypus variegatus*](https://en.wikipedia.org/wiki/Brown-throated_sloth), the Brown-throated sloth.  We'll go ahead and load the `ENMeval`, `dplyr` for data management, and [`spocc`](https://cran.r-project.org/package=spocc) packages (which we use to download occurrence records).

```{r occDownload}
library(spocc)
library(dplyr)
library(ENMeval)

# set a random seed for reproducibility
set.seed(48)

# Search GBIF for occurrence data.
# bv <- occ('Bradypus variegatus', 'gbif', limit=300, has_coords=TRUE)
occs <- readRDS("data/bvariegatus.rds")

# Get the latitude/coordinates for each locality. Also convert the tibble that occ() outputs
# to a data frame for compatibility with ENMeval functions.
# occs <- as.data.frame(bv$gbif$data$Bradypus_variegatus[,2:3])

# Remove duplicate rows (Note that you may or may not want to do this).
occs <- occs[!duplicated(occs),]
```

We are going to model the climatic niche suitability for our focal species using climate data from [WorldClim](http://www.worldclim.org/). WorldClim has a range of variables available at various resolutions; for simplicity, here we'll use the 9 bioclimatic variables at 10 arcmin resolution (about 20 km across at the equator) included in the `dismo` package. These climatic data are based on 50-year averages from 1950-2000. Now's also a good time to load the package, as it includes all the downstream dependencies (`raster`, `dismo`, etc.).

```{r envDownload, warning=FALSE, message=FALSE, fig.width = 5, fig.height = 5}
library(raster)

# Locate some predictor raster files from the dismo folder
files <- list.files(path=paste(system.file(package='dismo'), '/ex', sep=''), pattern='grd', full.names=TRUE)

# Read the raster files into a RasterStack
# We remove the "biome" categorical variable because categoricals can present a number of technical difficulties
# in many of the operations we do in this vignette, but ENMevaluate is fully compatible with categorical variables
# (as long as you declare them with argument "categoricals")
envs <- stack(files)[[-9]]

# Plot first raster in the stack, bio1
plot(envs[[1]], main=names(envs)[1])

# Add points for all the occurrence points onto the raster
points(occs)

# There are some points east on the Amazon. Let's say we know that this represents a subpopulation that we don't want to include in the model -- we can remove these points from the analysis by subsetting the occurrences by latitude and longitude
occs <- filter(occs, latitude > -20, longitude < -45)

# Plot the subsetted occurrences over the original ones to make sure we excluded the right ones
points(occs, col='red')

# As we will demonstrate model evaluation using testing data, we will specify a fake testing occurrence dataset
occs.testing <- data.frame(longitude = -runif(10, 55, 65), latitude = runif(10, 0, 5))
points(occs.testing, col="blue")
```


Now let's take a look at which areas of this extent are considerably different climatically with respect to environment associated with the occurrence points. To do this, we'll use the Multivariate Environmental Similarity surface, or MESS (see Elith et al. 2010 for details). Here we will use tools from the rmaxent because it also outputs maps of most similar and dissimilar variables, something that dismo::mess does not (it also avoids values of -Inf that dismo::mess often outputs). You'll need to use this code to install rmaxent, because it's not on CRAN: devtools::install_github('johnbaums/rmaxent'). We'll also use the rasterVis package because it maps categorical rasters better than base plotting functions, and the RColorBrewer package for plotting colors.

```{r}
library(rmaxent)
library(RColorBrewer)
library(rasterVis)
# First we extract the climatic variable values at the occurrence points -- these values are our "reference"
occs.z <- extract(envs, occs)
# Now we use the rmaxent function similarity to calculate environmental similarity metrics
# Just as a note, you cannot input categorical variables into this function or it won't work
occs.sim <- similarity(envs, occs.z)
# This is the MESS plot -- increasingly negative values represent increasingly different climatic
# conditions from the reference (our occurrences), while increasingly positive values are more similar
plot(occs.sim$similarity_min)
# Here we define some good colors for representing categorical variables
cols <- brewer.pal(9, "Set1")
# This map shows the variable for each grid cell that is most different from the reference
levelplot(occs.sim$mod, col.regions = cols, main = "Most different variable")
# This map shows the variable for each grid cell that is most similar to the reference
levelplot(occs.sim$mos, col.regions = cols, main = "Most similar variable")
```

Next, we will specify the background extent by cropping our global predictor variable rasters to a smaller region. Since our models will compare the environment at occurrence (i.e., presence) localities to the environment at background localities, we need to sample random points from a background extent. To help ensure we don't include areas that are suitable for our species but are unoccupied due to limitations like dispersal constraints, we will conservatively define the background extent as an area surrounding our occurrence localities. We will do this by buffering a bounding box that includes all occurrence localities. Some other methods of background extent delineation (e.g., minimum convex hulls) are more conservative because they better characterize the geographic space holding the points. In any case, this is one of the many things that you will need to carefully consider for your own study.

```{r backgExt, message=FALSE, warning=FALSE}
library(sf)
# library(sp)

# Make occs into a sf point object
occs.sf <- st_as_sf(occs, coords = c("longitude","latitude"), crs = 4326)
# Make sure the RasterStack has the same coordinate reference system (CRS) string
# Both are the same CRS, but when the strings are different, some spatial functions error
crs(envs) <- "+proj=longlat +datum=WGS84 +no_defs"
# occs.sf <- st_transform(occs.sf, "+proj=cea +lat_ts=0 +lon_0=-65.302734375")

# Buffer all occurrences by 5 degrees
# * this will give a warning because we are not buffering in a projected coordinate system
# * for simplicity, this vignette does not involve coordinate reference system (CRS) transformations,
# * but for a real analysis, transforming to a projected CRS before buffering is best practice
occs.buf <- st_buffer(occs.sf, dist = 5)
plot(envs[[1]], main = names(envs)[1])
plot(occs.buf, add = TRUE)

# Crop environmental rasters to match the study extent
envs.bg <- crop(envs, occs.buf)
# Next, mask the rasters to the shape of the buffers
envs.bg <- mask(envs.bg, occs.buf)
```

We may want to restrict the study extent to the continental areas (for example). We can use the simplified world map from the [`maptools`](https://cran.r-project.org/package=maptools) package and geoprocessing tools from the [`sf`](https://cran.r-project.org/package=sf) package to exclude the Caribbean from our study extent.

```{r removeCaribbean, message=FALSE, fig.width = 5, fig.height = 5, warning = FALSE}
library(maptools)

# Get a simple world countries polygon
data(wrld_simpl)

# Get polygons for Central and South America
central.amer <- wrld_simpl@data$SUBREGION==5
south.amer <- wrld_simpl@data$SUBREGION==13
# Subset wrld_simpl and convert to sf
ca.sa <- st_as_sf(wrld_simpl[central.amer | south.amer,])

# Buffer by 1 degree before masking to make sure you don't clip the coastline
ca.sa.buf <- st_buffer(ca.sa, dist = 1)

# Mask envs by the combined Central-South America polygon excluding the Caribbean
envs.bg <- mask(envs.bg, ca.sa.buf)

# Plot one of the new masked rasters -- we still have a few cells from the Galapagos, but are now missing the Caribbean
plot(envs.bg[[1]], main=names(envs.bg)[1])
points(occs)
```

In the next step, we'll sample 10,000 random points from the background (note that the number of background points is also a consideration you should make with respect to your own study).

```{r backgPts, fig.width = 5, fig.height = 5}
library(dismo)

# Randomly sample 10,000 background points from one background extent raster (only one per cell without replacement). Note: Since the raster has <10,000 pixels, you'll get a warning and all pixels will be used for background. We will be sampling from the biome variable because it is missing some grid cells, and we are trying to avoid getting background points with NA.
bg <- randomPoints(envs.bg[[9]], n=10000)
bg <- as.data.frame(bg)
colnames(bg) <- colnames(occs)

# Notice how we have pretty good coverage (every cell).
plot(envs.bg[[1]], legend=FALSE)
points(bg, col='red', pch=20, cex=0.2)
```

## Partitioning Occurrences for Evaluation {#partition}
A run of ENMevaluate begins by using one of six methods to partition occurrence localities into validation and training bins (folds) for *k*-fold cross-validation (Fielding and Bell 1997; Peterson et al. 2011). Data partitioning is done internally by `ENMevaluate()`, but can also be done externally with the partitioning functions. In this section, we explain and illustrate these different functions. We also demonstrate how to make informative plots of partitions and the environmental similarity of partitions to the background or projection extent.

1. [Spatial Block](#block)
2. [Spatial Checkerboard](#cb1)
3. [Spatial Hierarchical Checkerboard](#cb2)
4. [Jackknife (leave-one-out)](#jack)
5. [Random *k*-fold](#rand)
6. [Testing Data](#testing)
7. [User](#user)

The first three partitioning methods are variations of what Radosavljevic and Anderson (2014) referred to as 'masked geographically structured' data partitioning. Basically, these methods partition both occurrence records and background points into evaluation bins based on some spatial rules. The intention is to reduce spatial-autocorrelation between points that are included in the validation and training bins, which can overinflate model performance, at least for data sets that result from biased sampling (Veloz 2009; Hijmans 2012; Wenger and Olden 2012).

#### 1. Block {#block}
First, the 'block' method partitions data according to the latitude and longitude lines that divide the occurrence localities into four spatial groups of (insofar as possible) equal numbers. Both occurrence and background localities are assigned to each of the four bins based on their position with respect to these lines. The resulting object is a list of two vectors that supply the bin designation for each occurrence and background point.

```{r part.block, fig.width = 5, fig.height = 5}
block <- get.block(occs, bg)
# Let's make sure that we have an even number of occurrences in each partition
table(block$occs.grp)
# We can plot our partitions on one of our predictor variable rasters to visualize where they fall in space
evalplot.grps(pts = occs, pts.grp = block$occs.grp, envs = envs.bg[[1]])
# PLotting the background shows that the background extent is partitioned in a way that maximizes evenness
# of points across the four bins, not to maximize evenness of area
evalplot.grps(pts = bg, pts.grp = block$bg.grp, envs = envs.bg)
# If we are curious how different the environment associated with each partition is from that of all the others,
# we can use this function to plot histograms or rasters of MESS predictions with each partition as the reference
# It is clear that the block method results in partitions that can differ widely in their environmental representation
evalplot.grps.envSim(envs, occs, bg, occs.grp = block$occs.grp, bg.grp = block$bg.grp, plot.type = "histogram")
evalplot.grps.envSim(envs, occs, bg, occs.grp = block$occs.grp, bg.grp = block$bg.grp, plot.type = "raster")
```

#### 2. Checkerboard1 {#cb1}
The next two partitioning methods are variants of a 'checkerboard' approach to partition occurrence localities. These generate checkerboard grids across the study extent and partition the localities into groups based on where they fall on the checkerboard. In contrast to the block method, both checkerboard methods subdivide geographic space equally but do not ensure a balanced number of occurrence localities in each bin. For these methods, the user needs to provide a raster layer on which to base the underlying checkerboard pattern. Here we simply use the predictor variable RasterStack. Additionally, the user needs to define an *aggregation.factor*. This value tells the number of grids cells to aggregate when making the underlying checkerboard pattern.

The Checkerboard1 method partitions the points into *k* = 2 spatial groups using a simple checkerboard pattern.

```{r part.ck1, fig.width = 5, fig.height = 5}
cb1 <- get.checkerboard1(occs, envs.bg, bg, aggregation.factor=5)
evalplot.grps(pts = occs, pts.grp = cb1$occs.grp, envs = envs.bg)
# Plotting the background points shows the checkerboard pattern very clearly
evalplot.grps(pts = bg, pts.grp = cb1$bg.grp, envs = envs.bg)
# We can see from the MESS maps that this method results in similar
# environmental representation between the partitions
evalplot.grps.envSim(envs, occs, bg, occs.grp = cb1$occs.grp, bg.grp = cb1$bg.grp, plot.type = "histogram")
evalplot.grps.envSim(envs, occs, bg, occs.grp = cb1$occs.grp, bg.grp = cb1$bg.grp, plot.type = "raster")

# We can increase the aggregation factor to give the groups bigger boxes
cb1.large <- get.checkerboard1(occs, envs.bg, bg, aggregation.factor=30)
evalplot.grps(pts = occs, pts.grp = cb1.large$occs.grp, envs = envs.bg)
evalplot.grps(pts = bg, pts.grp = cb1.large$bg.grp, envs = envs.bg)
```

#### 3. Checkerboard2 {#cb2}
The Checkerboard2 method partitions the data into *k* = 4 spatial groups. This is done by hierarchically aggregating the input raster at two scales. Presence and background groups are assigned based on which box they fall on the hierarchical checkerboard.

```{r part.ck2, fig.width = 5, fig.height = 5}
cb2 <- get.checkerboard2(occs, envs.bg, bg, aggregation.factor=c(5,5))
evalplot.grps(pts = occs, pts.grp = cb2$occs.grp, envs = envs.bg)
# Plotting the background points shows the checkerboard pattern very clearly
evalplot.grps(pts = bg, pts.grp = cb2$bg.grp, envs = envs.bg)
# Different from checkerboard1, some partitions here do show some difference
# in environmental representation, but not as extreme as with block
evalplot.grps.envSim(envs, occs, bg, occs.grp = cb2$occs.grp, bg.grp = cb2$bg.grp, plot.type = "histogram")
evalplot.grps.envSim(envs, occs, bg, occs.grp = cb2$occs.grp, bg.grp = cb2$bg.grp, plot.type = "raster")
```

#### 4. Jackknife (leave-one-out) {#jack}
The next two methods differ from the first three in that (i) they do not partition the background points into different groups, and (ii) they do not account for spatial autocorrelation between validation and training localities. Primarily when working with relatively small data sets (e.g. < ca. 25 presence localities), users may choose a special case of *k*-fold cross-validation where the number of bins (*k*) is equal to the number of occurrence localities (*n*) in the data set (Pearson et al. 2007; Shcheglovitova and Anderson 2013). This is referred to as jackknife, or leave-one-out, partitioning.  As *n* models are processed with this partitioning method, the computation time could be long for large occurrence datasets. The background is not partitioned with jackknife.

```{r part.jk, fig.width = 5, fig.height = 5}
jack <- get.jackknife(occs, bg)
# If the number of input points is larger than 10, the legend for the groups is suppressed
evalplot.grps(pts = occs, pts.grp = jack$occs.grp, envs = envs.bg)
```

#### 5. Random k-fold {#rand}
The 'random k-fold' method partitions occurrence localities randomly into a user specified number of (*k*) bins. This method is equivalent to the 'cross-validate' partitioning scheme available in the current version of the Maxent software GUI. Especially with larger occurrence datasets, this partitioning method could randomly result in some spatial clustering of groups, which is why spatial partitioning methods are preferable for addressing spatial autocorrelation. Below, we partition the data into five random groups. The background is not partitioned with random k-fold.

```{r part.rand, fig.width = 5, fig.height = 5}
rand <- get.randomkfold(occs, bg, k = 5)
evalplot.grps(pts = occs, pts.grp = rand$occs.grp, envs = envs.bg)
# As the partitions are random, there is no clear environmental difference between them
evalplot.grps.envSim(envs, occs, bg, occs.grp = rand$occs.grp, bg.grp = rand$bg.grp, plot.type = "histogram")
evalplot.grps.envSim(envs, occs, bg, occs.grp = rand$occs.grp, bg.grp = rand$bg.grp, plot.type = "raster")
```

#### 6. Testing Data {#testing}: 
The 'testing' method evaluates the model on a testing dataset that is not used to create the full model (i.e., not included in the training data, or fully withheld), and thus cross validation statistics are not calculated. To illustrate this, we will make a table containing occurrences representing both training data and testing data and plot the partitions in the same way as above. However, the testing data (group 2) will not become training data for a new model. Instead, the training data (group 1) is used to make the model, and the testing data (group 2) are used only to evaluate it. Thus, the background extent does not include the testing data (a few points fall inside this extent because of the buffer we applied, but they are not used as training data).

```{r part.ind, fig.width = 5, fig.height = 5}
pts <- rbind(occs, occs.testing)
pts.grp <- c(rep(1, nrow(occs)), rep(2, nrow(occs.testing)))
evalplot.grps(pts = pts, pts.grp = pts.grp, envs = envs.bg)
# We use the same background groups as random partitions here and input the training data and
# testing dataset as though they were partitions of one big dataset
# We can see what is to be expected -- the testing dataset is much more restricted environmentally
# than the training data, and thus is much more different from the wider extent
evalplot.grps.envSim(envs, pts, bg, occs.grp = pts.grp, bg.grp = rand$bg.grp, plot.type = "histogram")
evalplot.grps.envSim(envs, pts, bg, occs.grp = pts.grp, bg.grp = rand$bg.grp, plot.type = "raster")
```

#### 7. User-defined {#user}
For maximum flexibility, the last partitioning method is designed so that users can define *a priori* partitions. This provides a flexible way to conduct spatially-independent cross-validation with background masking. For example, we demonstrate partitioning the occurrence data based on *k*-means groups. The user-defined partition option can also be used to input partition groups derived from other sources.

```{r part.user1, fig.width = 5, fig.height = 5}
grp.n <- 6
kmeans <- kmeans(occs, grp.n)
occs.grp <- kmeans$cluster
evalplot.grps(pts = occs, pts.grp = occs.grp, envs = envs.bg)
```

When using the user-defined partitioning method, we need to supply ENMevaluate with group identifiers for both occurrence points AND background points. If we want to use all background points for each group, we can set the background to zero.

```{r part.user2, fig.width = 5, fig.height = 5}
bg.grp <- rep(0, nrow(bg))
evalplot.grps(pts = bg, pts.grp = bg.grp, envs = envs.bg)
```

Alternatively, we may think of various ways to partition background data. This depends on the goals of the study but we might, for example, find it reasonable to partition background by clustering around the centroids of the occurrence clusters.

```{r part.user3, fig.width = 5, fig.height = 5}
centers <- kmeans$center
d <- pointDistance(bg, centers, lonlat=T)
bg.grp <- apply(d, 1, function(x) which(x==min(x)))
evalplot.grps(pts = bg, pts.grp = bg.grp, envs = envs.bg)
```

We can also use other packages to partition our data. As an example, we next show how to use the [`blockCV`](https://CRAN.R-project.org/package=blockCV) package to generate spatial partitions to input into ENMevaluate(). We use spatialBlock function in `blockCV` to generate blocks similar to the checkerboard partition in ENMeval, except that we choose here to select partitions randomly over these blocks. This package offers other kinds of block partitioning methods as well. Using other packages in this way expands the variety of partitions you can use to evaluate models in ENMeval, and we highly encourage experimenting with a plurality of methods.

```{r}
library(blockCV)
crs(envs)
occsBg.sf <- st_as_sf(rbind(occs, bg), coords = c("longitude","latitude"), crs = 4326)
# Here, we implement the spatialBlock function from the blockCV package
# The required inputs are similar to ENMeval partitioning functions, but here you assign
# the size of blocks in meters with the argument theRange (here set at 1000 km), and the 
# partition selection method can be assigned as either "checkerboard" or "random"
# In addition, the spatialBlock function returns a map showing the different spatial partitions
sb <- spatialBlock(speciesData = occsBg.sf, rasterLayer = envs.bg, theRange = 1000000, k = 5, selection = "random")
# Here, we can pull out the partition information from the SpatialBlock object to assemble
# occs.grp and bg.grp, which can be used for plotting or as user.grp inputs for ENMevaluate
occs.grp <- sb$foldID[1:nrow(occs)]
bg.grp <- sb$foldID[(nrow(occs)+1):length(sb$foldID)]
evalplot.grps(pts = bg, pts.grp = bg.grp, envs = envs.bg)
```

Choosing among these data partitioning methods depends on the research objectives and the characteristics of the study system. Refer to the [Resources](Resources) section for additional considerations on appropriate partitioning for evaluation.

## Running ENMeval {#eval}
Once you decide which method of data partitioning you would like to use, you are ready to start building models. We now move on to the main function in ENMeval: `ENMevaluate`.

- [Initial considerations](#eval.consid)
- [Different parameterizations](#eval.parameterizations)
- [Exploring the results (the ENMevaluate object)](#eval.explore)

#### Initial considerations {#eval.consid}
The two main parameters to define when calling `ENMevaluate` are (1) the range of regularization multiplier values and (2) the combinations of feature class to consider. The ***regularization multiplier*** (RM) determines the penalty for using variables or their transformations in the model. Higher RM values impose a stronger penalty on model complexity and thus result in simpler (*flatter*) model predictions. The ***feature classes*** determine the potential shape of the response curves. A model that is only allowed to include linear feature classes will most likely be simpler than a model that is allowed to include all possible feature classes. A more detailed description of these parameters is available in the [Resources](#resources) section. For the purposes of this vignette, we demonstrate simply how to adjust these parameters. The following section deals with comparing the outputs of each model.

Unless you supply the function with background points (which is recommended in many cases), you will need to define how many background points should be used with the 'n.bg' argument. If any of your predictor variables are categorical (e.g., biomes), you will need to define which layer(s) these are using the 'categoricals' argument.

ENMevaluate builds a separate model for each unique combination of RM values and feature class combinations. For example, the following call will build and evaluate 2 models. One with RM=1 and another with RM=2, both allowing only linear features.

<!-- ```{r load_vignette_data, echo = FALSE} -->
<!-- data(eval2) -->
<!-- ``` -->

```{r enmeval1, eval=FALSE}
e.mx.l <- ENMevaluate(occ = occs, env = envs, bg.coords = bg, algorithm = 'maxnet',
                             partitions = 'checkerboard2', tune.args = list(fc = "L", rm = 1:2))
```

We may, however, want to compare a wider range of models that can use a wider variety of feature classes and regularization multipliers:

```{r maxnet2, eval=TRUE}
e.mx.lqh <- ENMevaluate(occ = occs, env = envs, bg.coords = bg, algorithm = 'maxnet',
                             partitions = 'checkerboard2', tune.args = list(fc = c("L","LQ","LQH","H"), rm = 1:3))
```

When building many models, the command may take a long time to run. Of course this depends on the size of your dataset and the computer you are using. When working on big projects, running the command in parallel (`parallel=T`) can be faster. Note that running parallel can also be slower when working on small projects...

```{r maxnet2par, eval=FALSE}
e.mx.lqh <- ENMevaluate(occ = occs, env = envs, bg.coords = bg, algorithm = 'maxnet',
                             partitions = 'checkerboard2', tune.args = list(fc = c("L","LQ","LQH","H"), rm = 1:3),
                             parallel = TRUE)
```

Another way to save time at this stage is to turn off the option that generates model predictions across the full study extent (`rasterPreds=F`). Note, however, that the full model predictions are needed for calculating AICc values so those are returned as NA in the results table when the `rasterPreds` argument is set to NULL.

```{r maxnet2.noRas, eval=FALSE}
e.mx.lqh.nr <- ENMevaluate(occ = occs, env = envs, bg.coords = bg, algorithm = 'maxnet',
                             partitions = 'checkerboard2', tune.args = list(fc = c("L","LQ","LQH","H"), rm = 1:3),
                             rasterPreds = NULL)
```

We can also calculate one of two niche overlap statistics while running `ENMevaluate` by setting the arguments `overlap=TRUE` and `overlapStat`, which support Moran's I or Schoener's D (see [Warren et al. (2008)](https://dx.doi.org/10.1111/j.1558-5646.2008.00482.x)). Note that you can also calculate this value at a later stage using the separate `calc.niche.overlap()` function.

```{r maxnet2.overlap, results='hide'}
overlap <- calc.niche.overlap(e.mx.lqh@predictions, overlapStat="D")
overlap
```

#### Different parameterizations {#eval.parameterizations}
There are multiple ways to run the function `ENMevaluate()`, and we will go over how to specify each parameterization and what the effects are on the results.

```{r evalExamples, eval= F}
# This example is for Maxent models, and so we will specify the tune.tbl with ranges of feature classes and regularization multipliers
tune.args <- list(fc = c("L", "LQ", "H", "LQH"), rm = 1:5)

# 1. Standard: we are using the R package maxnet to avoid frequent user issues with rJava
e.mx <- ENMevaluate(occs, envs, bg, algorithm = "maxnet", tune.args = tune.args, categoricals = "biome", partitions = "block")
# If maxent.jar is installed and rJava loads properly, you can also run Maxent with the original Java software
e.mxjar <- ENMevaluate(occs, envs, bg, algorithm = "maxent.jar", tune.args = tune.args, categoricals = "biome", partitions = "block")

# 2. Testing partition: no cross validation statistics calculated; instead, model will be evaluated on a testing dataset that is not used to create the full model
e.test <- ENMevaluate(occs, envs, bg, algorithm = "maxnet", tune.args = tune.args, categoricals = "biome", partitions = "testing", occs.testing = occs.testing)

# 3. User partitions
user.grp <- list(occs.grp = round(runif(nrow(occs), 1, 2)), bg.grp = round(runif(nrow(bg), 1, 2)))
e.user <- ENMevaluate(occs, envs, bg, algorithm = "maxnet", tune.args = tune.args, categoricals = "biome", partitions = "user", user.grp = user.grp)

# 4. No partitions: no cross validation statistics calculated, nor any model evaluation on test data
e.noCV <- ENMevaluate(occs, envs, bg, algorithm = "maxnet", tune.args = tune.args, categoricals = "biome", partitions = "none")

# 5. No raster data (a.k.a, samples with data, or SWD): no full model raster predictions created, so will run faster; also, both cbi.train and cbi.val will be calculated on the point data (training and validation background) instead of on the "envs" rasters (default)
occs.z <- cbind(occs, raster::extract(envs, occs))
bg.z <- cbind(bg, raster::extract(envs, bg))
e.swd <- ENMevaluate(occs.z, bg = bg.z, algorithm = "maxnet", tune.args = tune.args, categoricals = "biome", partitions = "block")
```

You can also specify your own custom validation statistics for `ENMevaluate()` to calculate and report in the results tables. This is done by defining a custom function that has the arguments shown below, which are used by the ENMeval internal function `tune.validate()`. Here, we add functionality to calculate the AUC ratio and associated p-value for the partial ROC perfomance metric, defined in [Peterson et al. (2008)](http://dx.doi.org/10.1016/j.ecolmodel.2007.11.008) and implemented by the package [kuenm](http://dx.doi.org/10.7717/peerj.6281).

```{r}
# Define a custom function with these exact arguments, 
# They should provide you with the ingredients you need to specify most evaluation functions
# Make sure you return a data frame that specifies the names you want to see in the results tables
proc <- function(enm, occs.train.z, occs.val.z, bg.train.z, bg.val.z, mod.k, nk, other.settings, partitions) {
  proc <- kuenm::kuenm_proc(occs.val.pred, c(bg.train.pred, bg.val.pred))
  out <- data.frame(proc_auc_ratio = proc$pROC_summary[1], proc_pval = proc$pROC_summary[2], row.names = NULL)
  return(out)
}

# Now we can run ENMevaluate() with the argument "user.eval", and simply give it the custom function
e.mx.proc <- ENMevaluate(occs, envs, bg, algorithm = "maxnet", tune.args = tune.args, categoricals = "biome", partitions = "block", user.eval = proc)
# We can see the new performance statistic averages in the results table
head(e.mx.proc@results)
# Especially for the p-values, it likely makes more sense to look at the results tables for the partitions
head(e.mx.proc@results.partitions)
```

The last way to run `ENMevaluate()` is by specifying a new algorithm using the `ENMdetails` object. Built-in algorithms (maxent.jar, maxnet, BIOCLIM, randomForest, boostedRegressionTrees) are implemented as `ENMdetails` objects---they can be found in the /R folder of the package with the file name "enm.name", where "name" is the algorithm. The `ENMdetails` object specifies the way `ENMevaluate()` should run the algorithm. It has a some simple functions to define that output 1) the algorithm's name, 2) the function that runs the algorithm, 3) particular messages or errors, 4) the arguments for the model's function, 5) a specific parameterization of the `predict()` function, 6) the number of non-zero model coefficients, and 7) the variable importance table (if one is available from the model object). Users can also construct their own `ENMdetails` object using the built-in ones as guides. For example, a user can copy the "enm.maxnet.R" script, modify the code to specify a different model, save it as a new script in the /R folder with the name "enm.myAlgorithm", and use it to run `ENMevaluate()`.

```{r}
# If you saved a new script for "enm.myAlgorithm.R" and specified all the internal functions, enter it to the argument "user.enm"
# NOTE: It is important to make sure that your custom algorithm can handle categorical variables if you decide to specify them as predictors
# By default, ENMevaluate() removes categorical variables when running BIOCLIM models, but if you specify an algorithm that cannot use them,
# you will need to exclude them yourself before running
e.userENM <- ENMevaluate(occs, envs, bg, user.enm = enm.myAlgorithm, tune.args = tune.args, categoricals = "biome", partitions = "block")
```

#### Exploring the results {#eval.explore}
Now let's take a look at the output from `ENMevaluate` (which is an object of class `ENMevaluation`) in more detail (also see `?ENMevaluation`).  It contains the following slots:

- `algorithm` A character vector showing which algorithm was used
- `tune.settings` A data.frame of settings that were tuned
- `partition.method` A character of partitioning method used
- `results` A data.frame of evaluation summary statistics
- `results.partitions` A data.frame of evaluation k-fold statistics
- `models` A list of model objects
- `predictions` A RasterStack of model predictions
- `occ.pts` A data.frame of occurrence coordinates used for model training
- `occs.grp` A vector of partition groups for occurrence points
- `bg.pts` A data.frame of background coordinates used for model training
- `bg.grp` A vector of partition groups for background points
- `overlap` A list of matrices of pairwise niche overlap statistics

Let's first examine the structure of the object:
```{r results1}
e.mx

str(e.mx, max.level=3)
```

We can use helper functions to access the slots in the ENMevaluate object.
```{r algorithm}
# Access algorithm, tuning settings, and partition method information
eval.algorithm(e.mx)
eval.tune.settings(e.mx)
eval.partition.method(e.mx)
# Results table with summary statistics for cross validation on test data
eval.results(e.mx)
# Results table with cross validation statistics for each test partition
eval.results.partitions(e.mx)
# List of models with names corresponding to tune.args column label
eval.models(e.mx)
# The "betas" slot in a maxnet model is a named vector of the variable coefficients 
# and what kind they are (in R formula notation)
# Note that the html file that is created when maxent.jar is run is **not** kept
m1 <- eval.models(e.mx)[["LQH_1"]]
m1$betas
# For maxent.jar models, we can access this information in the lambdas slot
# The notation used here is difficult to decipher, so check out the [`rmaxent`](https://github.com/johnbaums/rmaxent/blob/master/) package
# available on Github for the `parse_lambdas()` function that makes it easier to read
m1.mxjar <- eval.models(eval.mxjar)[["LQH_1"]]
m1.mxjar@lambdas
# We can also get a long list of results statistics from the results slot
m1.mxjar@results
# RasterStack of model predictions (for extent of "envs") with names corresponding 
# to tune.args column label
eval.predictions(e.mx)
# Original occurrence data coordinates with associated predictor variable values
eval.occs(e.mx)
# Background data coordinates with associated predictor variable values
eval.bg(e.mx)
# Partition group assignments for occurrence data
eval.occs.grp(e.mx)
# Partition group assignments for background data
eval.bg.grp(e.mx)
```

## Plotting tuning results {#plot}
Plotting options in R are extremely flexible and here we demonstrate some key tools to explore the results of an ENMevaluate object graphically.

- [Plotting model predictions](#plot.preds)
- [Plotting response curves](#plot.resp)

ENMeval has a built-in ggplot-based plotting function (`eval.plot`) to visualize the results of the different models you tuned. Here, we will plot AICc and delta.AICc values for each feature class for both regularization multipliers.

```{r plot.res, fig.width = 5, fig.height = 5}
plot.eval(e = eval, stats = "or.mtp", col = "fc", x = "rm")
# We can plot more than one statistic at one with ggplot facetting
plot.eval(e = eval, stats = c("or.mtp", "auc.val"), col = "fc", x = "rm")
# Sometimes the error bars make it hard to visualize the plot, so we can try turning them off
plot.eval(e = eval, stats = c("or.mtp", "auc.val"), col = "fc", x = "rm", error.bars = FALSE)
# We can also fiddle with the dodge argument to jitter the positions of overlapping points
plot.eval(e = eval, stats = c("or.mtp", "auc.val"), col = "fc", x = "rm", dodge = 0.5)
# Finally, we can switch which variables are on the x-axis and which symbolized by color
# ENMeval currently only accepts two variables for plotting at a time
plot.eval(e = eval, stats = c("or.mtp", "auc.val"), col = "rm", x = "fc", error.bars = FALSE)
```

#### Model selection {#eval.select}

Once we have our results, we will want to select one or more models that we think are optimal across all the models we ran. For this example, we will demonstrate how to select models using AICc (which does not consider cross-validation results) and a sequential method that selects models with the lowest average test omission rate, and to break ties, next with the highest average validation AUC (this method uses cross-validation results).

```{r evaluations}
# Overall results
res <- eval.results(e.mx)
opt.aicc <- res %>% filter(delta.AICc == 0)
opt.aicc
opt.seq <- res %>% 
  filter(or.10p.avg == min(or.10p.avg)) %>% 
  filter(auc.val.avg == max(auc.val.avg))
opt.seq
```

Let's now pull the optimal model (using the sequential criteria) from the model list and examine it.
```{r mod.obj1}
mod.seq <- models(e.mx)[[opt.seq$tune.args]]
# The non-zero coefficients in our model
mod.seq$betas
# The response curves for our model
# maxent.jar models use the dismo::response() funtion for this
plot(mod.seq)
dev.off()
```

Now let's pull out and plot the prediction raster for our optimal model. Note that by default for maxent.jar or maxnet models, these predictions are in the 'cloglog' output format that is bounded between 0 and 1 (see Phillips et al. 2017 for more details). This can be changed with the pred.type argument in `ENMevaluate()`.
These predictions are for the entire extent of the input predictor variable rasters, and thus include areas outside of the background extent used for model training. Thus, we should interpret areas far outside this extent with caution.

```{r plotPred, fig.width = 5, fig.height = 5}
pred.seq <- eval.predictions(e.mx)[[opt.seq$tune.args]]
plot(pred.seq)
# Let's also plot the binned background points with the occurrence points on top
points(bg(e.mx), pch = 3, col = bg.grp(e.mx), cex = 0.5)
points(occs(e.mx), pch = 21, bg = occs.grp(e.mx))
```

Let's see how model complexity changes the predictions in our example. We'll compare the model built with only linear feature classes and the highest regularization multiplier value we used (i.e., fc='L', RM=5) with the model built with linear, quadratic, and hinge feature classes and the lowest regularization multiplier value we used (i.e., fc='LQH',  RM=1). We will first examine the response curves, and then the mapped model model predictions. Notice how the simpler models tend to have more smooth predictions of suitability, while the complex ones tend to show more patchiness. For more on simplicity and complexity in ENMs, see [Merow et al. 2014](https://doi.org/10.1111/ecog.00845).

```{r plot.pred2, fig.width = 5, fig.height = 2.5}
# First, let's examine the non-zero model coefficients in the betas slot
# The simpler model has fewer model coefficients
models(e.mx)[['L_5']]$betas
length(models(e.mx)[['L_5']]$betas)
models(e.mx)[['LQH_1']]$betas
length(models(e.mx)[['LQH_1']]$betas)
# Next, let's take a look at the response curves
# The complex model has responses with more curves (quadratic terms) and spikes (hinge terms)
plot(models(e.mx)[['L_5']])
plot(models(e.mx)[['LQH_1']])
# Finally, let's cut the plotting area into two rows to visualize the predictions side-by-side
par(mfrow=c(2,1), mar=c(2,1,2,0))
# The simplest model: linear features only and high regularization
plot(predictions(e.mx)[['L_5']], ylim=c(-30,20), xlim=c(-90,-30), legend=F, main='L_5 prediction')
# The most complex model: linear, quadratic, and hinge features with low regularization
plot(predictions(e.mx)[['LQH_1']], ylim=c(-30,20), xlim=c(-90,-30), legend=F, main='LQH_1 prediction')
```

## Null models

```{r}
mod.null <- ENMnullSims(eval, mod.settings = list(fc = "L", rm = 2), no.iter = 10, 
                     envs = envs, userStats.signs = list(maxTSS.val = 1, maxKappa.val = 1))
null.results(mod.null)
null.results.partitions(mod.null)
real.vs.null.results(mod.null)
```

#### Metadata {#metadata}
ENMeval now catalogs details of the analysis essential for reproducibility in a rangeModelMetadata object, accessible from the output ENMevaluation object. The framework and philosophy behind rangeModelMetadata is described in [Merow et al. 2019](https://doi.org/10.1111/geb.12993). 

```{r}
# fill in a RMM object based on the ENMevaluate object
rmm <- eval.rmm(e.mx1)
# Let's fill in our selection rules based on the sequential criteria we chose
rmm$model$selectionRules <- "lowest 10 percentile omission rate, break ties with average validation AUC"
# And enter our optimal model settings
rmm$model$finalModelSettings <- "LQ4"
# And then enter the details of our optimal model's prediction
rmm$prediction$continuous$minVal <- cellStats(pred.seq, min)
rmm$prediction$continuous$maxVal <- cellStats(pred.seq, max)
rmm$prediction$continuous$units <- "suitability (cloglog transformation)"
# This is just an example -- there may be more fields that you want to fill in
# Finally, save the metadata to CSV
rangeModelMetadata::rmmToCSV(rmm, "rmm_mx1.csv")
```

## Downstream Analyses (*under construction*) {#downstream}
Below is a running list of other things we plan to add to this vignette.  Feel free to let us know if there are particular things you would like to see added.

- Working with the output from the 'maxnet' algorithm
- Extracting model results from object (various thresholds)
- Do MESS map (Use `mess()` in the `dismo` package)

## Resources (*under construction*) {#resources}

###### Web Resources
- [Hijmans, R. and Elith, J. (2016) Species distribution modeling with R. dismo vignette.](https://cran.r-project.org/package=dismo)

- [Phillips, S. J. (2006) Phillips, S. (2006) A brief tutorial on Maxent. AT&T Research. Available at: http://biodiversityinformatics.amnh.org/open_source/maxent/](http://biodiversityinformatics.amnh.org/open_source/maxent/)

- [Yoder, J. (2013) Species distribution models in R. The Molecular Ecologist.](http://www.molecularecologist.com/2013/04/species-distribution-models-in-r/)

- [Maxent Google Group](https://groups.google.com/forum/embed/#!forum/maxent)

###### General Guides
- [Merow, C., Smith, M., and Silander, J.A. (2013) A practical guide to Maxent: what it does, and why inputs and settings matter. Ecography 36, 1-12.](http://onlinelibrary.wiley.com/doi/10.1111/j.1600-0587.2013.07872.x/abstract)

- [Peterson, A.T., Soberón, J., Pearson, R.G., Anderson, R.P., Martínez-Meyer, E., Nakamura, M., and Araújo, M.B. (2011) Ecological Niches and Geographic Distributions. Monographs in Population Biology, 49. Princeton University Press.](http://press.princeton.edu/titles/9641.html)

- [Renner, I.W., Elith, J., Baddeley, A., Fithian, W., Hastie, T., Phillips, S.J., . . . Warton, D.I. (2015) Point process models for presence-only analysis. Methods in Ecology and Evolution 6, 366-379.](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12352/abstract)

###### Environmental Similarity
= [Elith, J., Kearney, M., & Phillips, S. (2010). The art of modelling range‐shifting species. Methods in ecology and evolution, 1(4), 330-342.](https://doi.org/10.1111/j.2041-210X.2010.00036.x)

- [Owens, H. L., Campbell, L. P., Dornak, L. L., Saupe, E. E., Barve, N., Soberón, J., ... & Peterson, A. T. (2013). Constraints on interpretation of ecological niche models by limited environmental ranges on calibration areas. Ecological Modelling, 263, 10-18.](https://doi.org/10.1016/j.ecolmodel.2013.04.011)

###### Model Evaluation
- [Aiello-Lammens, M.E., Boria, R.A., Radosavljevic, A., Vilela, B., and Anderson, R.P. (2015) spThin: an R package for spatial thinning of species occurrence records for use in ecological niche models. Ecography 38, 541-545.](http://onlinelibrary.wiley.com/doi/10.1111/ecog.01132/abstract)

- [Fielding, A.H. and Bell, J.F. (1997) A review of methods for the assessment of prediction errors in conservation presence-absence models. Environmental Conservation 24, 38-49.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.463.359&rep=rep1&type=pdf)

- [Hijmans, R.J. (2012) Cross-validation of species distribution models: removing spatial sorting bias and calibration with a null model. Ecology 93, 679-688.](http://onlinelibrary.wiley.com/doi/10.1890/11-0826.1/abstract)

- [Muscarella, R., Galante, P. J., Soley-Guardia, M., Boria, R. A., Kass, J. M., Uriarte, M. and Anderson, R. P. (2014), ENMeval: An R package for conducting spatially independent evaluations and estimating optimal model complexity for Maxent ecological niche models. Methods Ecol Evol, 5: 1198–1205.](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12261/full)

- [Radosavljevic, A. and Anderson, R.P. (2014) Making better Maxent models of species distributions: complexity, overfitting and evaluation. Journal of Biogeography 41, 629-643.](http://onlinelibrary.wiley.com/doi/10.1111/jbi.12227/abstract)

- [Shcheglovitova, M. and Anderson, R.P. (2013) Estimating optimal complexity for ecological niche models: A jackknife approach for species with small sample sizes. Ecol. Model. 269, 9-17.](http://www.sciencedirect.com/science/article/pii/S0304380013004043)

- [Veloz, S.D. (2009) Spatially autocorrelated sampling falsely inflates measures of accuracy for presence-only niche models. Journal of Biogeography 36, 2290-2299.](http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2699.2009.02174.x/abstract)

- [Wenger, S.J. and Olden, J.D. (2012) Assessing transferability of ecological models: an underappreciated aspect of statistical validation. Methods in Ecology and Evolution 3, 260-267.](http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00170.x/abstract)

###### Some Empirical Examples
- [Pearson, R.G., Raxworthy, C.J., Nakamura, M., and Peterson, A.T. (2007) Predicting species distributions from small numbers of occurrence records: a test case using cryptic geckos in Madagascar. Journal of Biogeography 34, 102-117.](http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2699.2006.01594.x/abstract)


