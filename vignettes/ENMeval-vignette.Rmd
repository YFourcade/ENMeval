---
title: "ENMeval v1.9.0 Vignette"
author: "Jamie M. Kass, Robert Muscarella, and Peter J. Galante"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ENMeval v1.0.0 Vignette}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, echo = FALSE, include=FALSE}
library(knitr)
knitr::opts_chunk$set(collapse=TRUE, message=FALSE, warning=FALSE, eval=TRUE, comment="#>", fig.width = 5, fig.height = 5)
```

- [Introduction](#intro)
- [Data Acquisition & Pre-processing](#data)
- [Partitioning Occurrences for Evaluation](#partition)
- [Running ENMeval](#eval)
- [Plotting results](#plot)
- [Downstream Analyses](#downstream)
- [Resources](#resources)


## Introduction {#intro}

[`ENMeval`](https://cran.r-project.org/package=ENMeval) is an R package that performs automated tuning and evaluations of ecological niche models (ENMs, a.k.a. species distribution models or SDMs), which can estimate species' ranges and niche characteristics using data on species occurrences and environmental variables. 

Some of the most frequently used ENMs are machine learning algorithms with settings that can be "tuned" to determine optimal levels of model complexity. In implementation, this means building models of varying settings, then evaluating them and comparing their performance to select the optimal settings. Such tuning exercises can result in models that balance goodness-of-fit (i.e., avoiding overfitting) and predictive ability. Model evaluation is often done with cross validation, which consists of partitioning the data into groups, building a model with all the groups but one, evaluating this model on the left-out group, then repeating the process until all groups have been left out once.

The primary function `ENMevaluate` runs the full analysis and returns an `ENMevaluation` object, which contains various results including a table of evaluation statistics and model objects and prediction rasters for each combination of model settings. The package also features functionality for adding and customizing ENM algorithms and evaluations, automatic generation of metadata, a null model for calculating effect size and significance of performance metrics, various plotting tools, options for parallel computing, and more. 

For a detailed description of the original `ENMeval` 0.1.0, check out the open-access publication:
[Muscarella, R., Galante, P. J., Soley-Guardia, M., Boria, R. A., Kass, J. M., Uriarte, M. and Anderson, R. P. (2014), ENMeval: An R package for conducting spatially independent evaluations and estimating optimal model complexity for Maxent ecological niche models. Methods in Ecology and Evolution, 5: 1198–1205.](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12261/full)

Although older versions (0.3.0 and earlier) implemented only the ENM presence-only algorithm Maxent through the [Java software](http://biodiversityinformatics.amnh.org/open_source/maxent/) and R with the package  [`maxnet`](https://cran.r-project.org/package=maxnet), this new version 1.9.0 can implement any ENM algorithm by specifying its settings in a *ENMdetails* object (see below for an example). 

## Data Acquisition & Pre-processing {#data}
In this vignette, we briefly demonstrate acquisition and pre-processing of input data for `ENMeval`. There are a number of other excellent tutorials on these steps, some of which we compiled in the [Resources](#resources) section.

We'll start by downloading an occurrence dataset for [*Bradypus variegatus*](https://en.wikipedia.org/wiki/Brown-throated_sloth), the Brown-throated sloth.  We'll go ahead and load  `ENMeval` and other packages we'll be using in the vignette.

```{r occDownload}
# Load packages -- the order here is important because some pkg functions overwrite others
library(ENMeval)
library(dismo)
library(dplyr)
library(RColorBrewer)
library(ggplot2)
library(rasterVis)
library(sf)

# Set a random seed in order to be able to reproduce this analysis
set.seed(48)

# You can search online databases like GBIF using the spocc package (commented below),
# but here we will load in some pre-downloaded data
# bv <- spocc::occ('Bradypus variegatus', 'gbif', limit=300, has_coords=TRUE)
# occs <- as.data.frame(bv$gbif$data$Bradypus_variegatus[,2:3])
occs <- readRDS("bvariegatus.rds")

# Remove duplicate rows - doesn't change our data, but is good practice in general
occs <- occs[!duplicated(occs),]
```

We are going to model the climatic niche suitability for our focal species using climate data from [WorldClim](http://www.worldclim.org/). WorldClim has a range of variables available at various resolutions; for simplicity, here we'll use the 9 bioclimatic variables at 10 arcmin resolution (about 20 km across at the equator) included in the `dismo` package from WorldClim 1.4. These climatic data are based on 50-year averages from 1950-2000.

```{r envDownload, warning=FALSE, message=FALSE}
# Locate the predictor raster files from the dismo folder
envs.files <- list.files(path=paste(system.file(package='dismo'), '/ex', sep=''), pattern='grd', full.names=TRUE)

# Read the raster files into a RasterStack
# These variables represent 8 bioclimatic variables and one categorical variable "biome"
# Find the descriptions of the bioclimatic variables here: https://www.worldclim.org/data/bioclim.html
envs <- stack(envs.files)
# The biome raster has some NAs for cells that have values in the other rasters
# Let's mask all rasters to biome to change the value of these cells to NA for all rasters
# ENMeval will do this automatically, but let's do it here to avoid the warning message later
# We change back from a RasterBrick to RasterStack because of issues with assigning factor rasters
# for RasterBricks
envs <- mask(envs, envs[[9]]) %>% stack()
# Make sure to declare the categorical variable as a factor
envs$biome <- as.factor(envs$biome)
# Let's now remove occurrences that are cell duplicates
# ALthough Maxent does this by default, keep in mind that for other algorithms you may
# or may not want to do this based on the aims of your study
occs.cells <- extract(envs[[1]], occs, cellnumbers = TRUE)
occs.cellDups <- duplicated(occs.cells[,1])
occs <- occs[!occs.cellDups,]

# Plot first raster in the stack, bio1 (mean annual temperature)
plot(envs[[1]], main="Mean annual temperature")

# Add points for all the occurrence points onto the raster
points(occs)

# There are some points east of the Amazon River 
# Suppose we know this is a population that we don't want to include in the model
# We can remove these points from the analysis by subsetting the occurrences by latitude and longitude
occs <- filter(occs, latitude > -20, longitude < -45)

# Plot the subsetted occurrences to make sure we filtered correctly
points(occs, col = 'red')

# We will demonstrate model evaluation using fully-withheld testing data later, so now we will specify a fake testing occurrence dataset
occs.testing <- data.frame(longitude = -runif(10, 55, 65), latitude = runif(10, -10, 0))
points(occs.testing, col="blue")
```

Now let's take a look at which areas of this extent are climatically different areas associated with the occurrence points. To do this, we'll use the Multivariate Environmental Similarity surface, or MESS (see Elith et al. 2010 for details). Higher positive values indicate increasing similarity, while higher negative values indicate dissimilarity. Although the `mess()` function from the dismo package is frequently used, here we will use tools from the package [`rmaxent`](https://github.com/johnbaums/rmaxent) because it also plots maps of the most similar and dissimilar variables (and it also avoids values of -Inf that dismo::mess often outputs). We will demonstrate the `ENMeval` plotting functions for environmental similarity later.

```{r}
# First we extract the climatic variable values at the occurrence points -- these values are our "reference"
# We remove the categorical variable for these operations because the math is for continuous variables
occs.z <- extract(envs[[-9]], occs)
# Now we use the similarity() function (borrowed from the rmaxent package) to calculate environmental similarity metrics
# Just as a note, you cannot input categorical variables into this function or it won't work
occs.sim <- rmaxent::similarity(envs[[-9]], occs.z)
occs.mess <- occs.sim$similarity_min
# This is the MESS plot -- increasingly negative values represent increasingly different climatic
# conditions from the reference (our occurrences), while increasingly positive values are more similar
# First, we'll make a SpatialPoints object for our occurrences for plotting with levelplot from the rasterVis package
# This package has great plotting functionality for rasters, and by default bins values for display when data is continuous
occs.sp <- SpatialPoints(occs)
# Vector data (points, polygons) are added to a levelplot with a "+", like ggplot
levelplot(occs.mess, main = "Environmental similarity", margin = FALSE) + layer(sp.points(occs.sp, col="black"))
# Continuous plotting can be done as demonstrated below by specifiying a scale
myScale <- seq(cellStats(occs.mess, min), cellStats(occs.mess, max), length.out = 100)
levelplot(occs.mess, main = "Environmental similarity", at = myScale, margin = FALSE) + layer(sp.points(occs.sp, col="black"))
# Here we define some good colors for representing categorical variables
cols <- brewer.pal(8, "Set1")
# This map shows the variable for each grid cell that is most different from the reference
levelplot(occs.sim$mod, col.regions = cols, main = "Most different variable") + layer(sp.points(occs.sp, col="black"))
# This map shows the variable for each grid cell that is most similar to the reference
levelplot(occs.sim$mos, col.regions = cols, main = "Most similar variable") + layer(sp.points(occs.sp, col="black"))
```

Next, we will specify the background extent by cropping our global predictor variable rasters to a smaller region. Since our models will compare the environment at occurrence (i.e., presence) localities to the environment at background localities, we need to sample random points from a background extent. To help ensure we don't include areas that are suitable for our species but are unoccupied due to limitations like dispersal constraints, we will conservatively define the background extent as an area surrounding our occurrence localities. We will do this by buffering a bounding box that includes all occurrence localities. Some other methods of background extent delineation (e.g., minimum convex hulls) are more conservative because they better characterize the geographic space holding the points. In any case, this is one of the many things that you will need to carefully consider for your own study.

```{r backgExt, message=FALSE, warning=FALSE}
# We'll now experiment with a different spatial R package called sf (simple features)
# Let's make our occs into a sf object -- here we specify the coordinate reference system (crs) 
# in order to perform spatial functions
occs.sf <- st_as_sf(occs, coords = c("longitude","latitude"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
# Make sure the RasterStack has the same coordinate reference system (CRS) string
# Both are the same CRS, but when the strings are different, some spatial functions error
crs(envs) <- crs(occs.sf)

# Buffer all occurrences by 7 degrees: this will give a warning because we are not buffering 
# in a projected coordinate system, then union the polygons together (for visualization), and convert
# back to a form that the raster package can use
# We choose 7 degrees here to avoid sampling the Caribbean islands
# For simplicity, this vignette does not involve coordinate reference system (CRS) transformations,
# but for a real analysis, transforming to a projected CRS before buffering is best practice
occs.buf <- st_buffer(occs.sf, dist = 7) %>% st_union() %>% st_sf()
plot(envs[[1]], main = names(envs)[1])
points(occs)
# To add sf objects to a plot, use add = TRUE
plot(occs.buf, border = "blue", lwd = 3, add = TRUE)

# Crop environmental rasters to match the study extent
envs.bg <- crop(envs, occs.buf)
# Next, mask the rasters to the shape of the buffers
envs.bg <- mask(envs.bg, occs.buf)
plot(envs.bg[[1]], main = names(envs)[1])
points(occs)
plot(occs.buf, border = "blue", lwd = 3, add = TRUE)
```

In the next step, we'll sample 10,000 random points from the background (note that the number of background points is also a consideration you should make with respect to your own study).

```{r backgPts}
# Randomly sample 10,000 background points from one background extent raster (only one per cell without replacement). Note: Since the raster has <10,000 pixels, you'll get a warning and all pixels will be used for background. We will be sampling from the biome variable because it is missing some grid cells, and we are trying to avoid getting background points with NA.
bg <- randomPoints(envs.bg[[9]], n = 10000) %>% as.data.frame()
colnames(bg) <- colnames(occs)

# Notice how we have pretty good coverage (every cell).
plot(envs.bg[[1]])
points(bg, pch = 20, cex = 0.2)
```

## Partitioning Occurrences for Evaluation {#partition}
A run of ENMevaluate begins by using one of six methods to partition occurrence localities into validation and training bins (folds) for *k*-fold cross-validation (Fielding and Bell 1997; Peterson et al. 2011). Data partitioning is done internally by `ENMevaluate()`, but can also be done externally with the partitioning functions. In this section, we explain and illustrate these different functions. We also demonstrate how to make informative plots of partitions and the environmental similarity of partitions to the background or projection extent.

1. [Spatial Block](#block)
2. [Spatial Checkerboard](#cb1)
3. [Spatial Hierarchical Checkerboard](#cb2)
4. [Jackknife (leave-one-out)](#jack)
5. [Random *k*-fold](#rand)
6. [Testing Data](#testing)
7. [User](#user)

The first three partitioning methods are variations of what Radosavljevic and Anderson (2014) referred to as 'masked geographically structured' data partitioning. Basically, these methods partition both occurrence records and background points into evaluation bins based on some spatial rules. The intention is to reduce spatial-autocorrelation between points that are included in the validation and training bins, which can overinflate model performance, at least for data sets that result from biased sampling (Veloz 2009; Hijmans 2012; Wenger and Olden 2012).

#### 1. Block {#block}
First, the 'block' method partitions data according to the latitude and longitude lines that divide the occurrence localities into four spatial groups of (insofar as possible) equal numbers. Both occurrence and background localities are assigned to each of the four bins based on their position with respect to these lines. The resulting object is a list of two vectors that supply the bin designation for each occurrence and background point.

```{r part.block}
block <- get.block(occs, bg)
# Let's make sure that we have an even number of occurrences in each partition
table(block$occs.grp)
# We can plot our partitions on one of our predictor variable rasters to visualize where they fall in space
# The ENMeval plotting functions use ggplot2, a widely popular plotting package for R with many online resources
# We can add to the ggplots with other ggplot functions in an additive way, making these plots easily customizable
evalplot.grps(pts = occs, pts.grp = block$occs.grp, envs = envs.bg) + ggtitle("Spatial block partitions: occurrences")
# PLotting the background shows that the background extent is partitioned in a way that maximizes evenness
# of points across the four bins, not to maximize evenness of area
evalplot.grps(pts = bg, pts.grp = block$bg.grp, envs = envs.bg) + ggtitle("Spatial block partitions: background")
# If we are curious how different the environment associated with each partition is from that of all the others,
# we can use this function to plot histograms or rasters of MESS predictions with each partition as the reference
# It is clear that the block method results in partitions that can differ widely in their environmental representation

# First we need to extract the predictor variable values at our occurrence and background localities
occs.z <- cbind(occs, raster::extract(envs, occs))
bg.z <- cbind(bg, raster::extract(envs, bg))
evalplot.envSim.hist(sim.type = "mess", ref.data = "occs", occs.z = occs.z, bg.z = bg.z, occs.grp = block$occs.grp, bg.grp = block$bg.grp, categoricals = "biome")
evalplot.envSim.hist(sim.type = "most_diff", ref.data = "occs", occs.z = occs.z, bg.z = bg.z, occs.grp = block$occs.grp, bg.grp = block$bg.grp, categoricals = "biome")
evalplot.envSim.hist(sim.type = "most_sim", ref.data = "occs", occs.z = occs.z, bg.z = bg.z, occs.grp = block$occs.grp, bg.grp = block$bg.grp, categoricals = "biome")

# Here we plot environmental similarity values for the entire extent with respect to each validation group
# We use the bb.buf (bounding box buffer) parameter to zoom in to our study extent
evalplot.envSim.map(sim.type = "mess", ref.data = "occs", envs = envs, occs.z = occs.z, bg.z = bg.z, occs.grp = block$occs.grp, bg.grp = block$bg.grp, categoricals = "biome", bb.buf = 7)
evalplot.envSim.map(sim.type = "most_diff", ref.data = "occs", envs = envs, occs.z = occs.z, bg.z = bg.z, occs.grp = block$occs.grp, bg.grp = block$bg.grp, categoricals = "biome", bb.buf = 7)
evalplot.envSim.map(sim.type = "most_sim", ref.data = "occs", envs = envs, occs.z = occs.z, bg.z = bg.z, occs.grp = block$occs.grp, bg.grp = block$bg.grp, categoricals = "biome", bb.buf = 7)
```

#### 2. Checkerboard1 {#cb1}
The next two partitioning methods are variants of a 'checkerboard' approach to partition occurrence localities. These generate checkerboard grids across the study extent and partition the localities into groups based on where they fall on the checkerboard. In contrast to the block method, both checkerboard methods subdivide geographic space equally but do not ensure a balanced number of occurrence localities in each bin. For these methods, the user needs to provide a raster layer on which to base the underlying checkerboard pattern. Here we simply use the predictor variable RasterStack. Additionally, the user needs to define an *aggregation.factor*. This value tells the number of grids cells to aggregate when making the underlying checkerboard pattern.

The Checkerboard1 method partitions the points into *k* = 2 spatial groups using a simple checkerboard pattern.

```{r part.ck1}
cb1 <- get.checkerboard1(occs, envs.bg, bg, aggregation.factor=5)
evalplot.grps(pts = occs, pts.grp = cb1$occs.grp, envs = envs.bg)
# Plotting the background points shows the checkerboard pattern very clearly
evalplot.grps(pts = bg, pts.grp = cb1$bg.grp, envs = envs.bg)
# We can see from the MESS maps that this method results in similar
# environmental representation between the partitions
evalplot.envSim.hist(sim.type = "mess", ref.data = "occs", occs.z = occs.z, bg.z = bg.z, occs.grp = cb1$occs.grp, bg.grp = cb1$bg.grp, categoricals = "biome")
evalplot.envSim.map(sim.type = "mess", ref.data = "occs", envs = envs, occs.z = occs.z, bg.z = bg.z, occs.grp = cb1$occs.grp, bg.grp = cb1$bg.grp, categoricals = "biome", bb.buf = 7)
# We can increase the aggregation factor to give the groups bigger boxes
# This can result in groups that are more environmentally different from each other
cb1.large <- get.checkerboard1(occs, envs.bg, bg, aggregation.factor=30)
evalplot.grps(pts = occs, pts.grp = cb1.large$occs.grp, envs = envs.bg)
evalplot.grps(pts = bg, pts.grp = cb1.large$bg.grp, envs = envs.bg)
evalplot.envSim.hist(sim.type = "mess", ref.data = "occs", occs.z = occs.z, bg.z = bg.z, occs.grp = cb1.large$occs.grp, bg.grp = cb1$bg.grp, categoricals = "biome")
evalplot.envSim.map(sim.type = "mess", ref.data = "occs", envs = envs, occs.z = occs.z, bg.z = bg.z, occs.grp = cb1.large$occs.grp, bg.grp = cb1$bg.grp, categoricals = "biome", bb.buf = 7)
```

#### 3. Checkerboard2 {#cb2}
The Checkerboard2 method partitions the data into *k* = 4 spatial groups. This is done by hierarchically aggregating the input raster at two scales. Presence and background groups are assigned based on which box they fall on the hierarchical checkerboard.

```{r part.ck2}
cb2 <- get.checkerboard2(occs, envs.bg, bg, aggregation.factor=c(5,5))
evalplot.grps(pts = occs, pts.grp = cb2$occs.grp, envs = envs.bg)
# Plotting the background points shows the checkerboard pattern very clearly
evalplot.grps(pts = bg, pts.grp = cb2$bg.grp, envs = envs.bg)
# Different from checkerboard1, some partitions here do show some difference
# in environmental representation, but not as consistently different as with block
evalplot.envSim.hist(sim.type = "mess", ref.data = "occs", occs.z = occs.z, bg.z = bg.z, occs.grp = cb2$occs.grp, bg.grp = cb2$bg.grp, categoricals = "biome")
evalplot.envSim.map(sim.type = "mess", ref.data = "occs", envs = envs, occs.z = occs.z, bg.z = bg.z, occs.grp = cb2$occs.grp, bg.grp = cb2$bg.grp, categoricals = "biome", bb.buf = 7)
```

#### 4. Jackknife (leave-one-out) {#jack}
The next two methods differ from the first three in that (i) they do not partition the background points into different groups, and (ii) they do not account for spatial autocorrelation between validation and training localities. Primarily when working with relatively small data sets (e.g. < ca. 25 presence localities), users may choose a special case of *k*-fold cross-validation where the number of bins (*k*) is equal to the number of occurrence localities (*n*) in the data set (Pearson et al. 2007; Shcheglovitova and Anderson 2013). This is referred to as jackknife, or leave-one-out, partitioning.  As *n* models are processed with this partitioning method, the computation time could be long for large occurrence datasets. The background is not partitioned with jackknife.

```{r part.jk}
jack <- get.jackknife(occs, bg)
# If the number of input points is larger than 10, the legend for the groups is suppressed
evalplot.grps(pts = occs, pts.grp = jack$occs.grp, envs = envs.bg)
```

#### 5. Random k-fold {#rand}
The 'random k-fold' method partitions occurrence localities randomly into a user specified number of (*k*) bins. This method is equivalent to the 'cross-validate' partitioning scheme available in the current version of the Maxent software GUI. Especially with larger occurrence datasets, this partitioning method could randomly result in some spatial clustering of groups, which is why spatial partitioning methods are preferable for addressing spatial autocorrelation. Below, we partition the data into five random groups. The background is not partitioned with random k-fold.

```{r part.rand}
rand <- get.randomkfold(occs, bg, k = 5)
evalplot.grps(pts = occs, pts.grp = rand$occs.grp, envs = envs.bg)
# As the partitions are random, there is no large environmental difference between them
evalplot.envSim.hist(sim.type = "mess", ref.data = "occs", occs.z = occs.z, bg.z = bg.z, occs.grp = rand$occs.grp, bg.grp = rand$bg.grp, categoricals = "biome")
evalplot.envSim.map(sim.type = "mess", ref.data = "occs", envs = envs, occs.z = occs.z, bg.z = bg.z, occs.grp = rand$occs.grp, bg.grp = rand$bg.grp, categoricals = "biome", bb.buf = 7)
```

#### 6. Testing Data {#testing}: 
The 'testing' method evaluates the model on a testing dataset that is not used to create the full model (i.e., not included in the training data, or fully withheld), and thus cross validation statistics are not calculated. To illustrate this, we will make a table containing occurrences representing both training data and testing data and plot the partitions in the same way as above. However, the testing data (group 2) will not become training data for a new model. Instead, the training data (group 1) is used to make the model, and the testing data (group 2) are used only to evaluate it. Thus, the background extent does not include the testing data (a few points fall inside this extent because of the buffer we applied, but they are not used as training data).

```{r part.ind}
# First, let's plot the testing points with the rest of our data
evalplot.grps(pts = rbind(occs, occs.testing), pts.grp = c(rep(1, nrow(occs)), rep(2, nrow(occs.testing))), envs = envs.bg)
# Next, let's extract the predictor variable values for our testing points
occs.testing.z <- cbind(occs.testing, raster::extract(envs, occs.testing))
# We use the same background groups as random partitions here because the background used for testing data 
# is also from the full study extent. We use here the occs.testing.z parameter to add information for
# our testing localities, and we set the partitions for occurrences all to zero (as no partitioning is done).
evalplot.envSim.hist(sim.type = "mess", ref.data = "occs", occs.z = occs.z, bg.z = bg.z, occs.grp = rep(0, nrow(occs)), bg.grp = rand$bg.grp, categoricals = "biome", occs.testing.z = occs.testing.z)
evalplot.envSim.map(sim.type = "mess", ref.data = "occs", envs = envs, occs.z = occs.z, bg.z = bg.z, occs.grp = rep(0, nrow(occs)), bg.grp = rand$bg.grp, categoricals = "biome", bb.buf = 7, occs.testing.z = occs.testing.z)
# We can see what is to be expected -- the testing dataset is much more restricted environmentally
# than the training data, and thus is much more different from the wider extent
```

#### 7. User-defined {#user}
For maximum flexibility, the last partitioning method is designed so that users can define *a priori* partitions. This provides a flexible way to conduct spatially-independent cross-validation with background masking. For example, we demonstrate partitioning the occurrence data based on *k*-means groups. The user-defined partition option can also be used to input partition groups derived from other sources.

```{r part.user1}
grp.n <- 6
kmeans <- kmeans(occs, grp.n)
occs.grp <- kmeans$cluster
evalplot.grps(pts = occs, pts.grp = occs.grp, envs = envs.bg)
```

When using the user-defined partitioning method, we need to supply ENMevaluate with group identifiers for both occurrence points AND background points. If we want to use all background points for each group, we can set the background to zero.

```{r part.user2}
bg.grp <- rep(0, nrow(bg))
evalplot.grps(pts = bg, pts.grp = bg.grp, envs = envs.bg)
```

Alternatively, we may think of various ways to partition background data. This depends on the goals of the study but we might, for example, find it reasonable to partition background by clustering around the centroids of the occurrence clusters.

```{r part.user3}
centers <- kmeans$center
d <- pointDistance(bg, centers, lonlat=T)
bg.grp <- apply(d, 1, function(x) which(x==min(x)))
evalplot.grps(pts = bg, pts.grp = bg.grp, envs = envs.bg)
```

We can also use other packages to partition our data. As an example, we next show how to use the [`blockCV`](https://CRAN.R-project.org/package=blockCV) package to generate spatial partitions to input into ENMevaluate(). We use spatialBlock function in `blockCV` to generate blocks similar to the checkerboard partition in ENMeval, except that we choose here to select partitions randomly over these blocks. This package offers other kinds of block partitioning methods as well. Using other packages in this way expands the variety of partitions you can use to evaluate models in ENMeval, and we highly encourage experimenting with a plurality of methods.

```{r}
library(blockCV)

occsBg.sf <- st_as_sf(rbind(occs, bg), coords = c("longitude","latitude"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
crs(envs.bg) <- crs(occsBg.sf)
# Here, we implement the spatialBlock function from the blockCV package
# The required inputs are similar to ENMeval partitioning functions, but here you assign
# the size of blocks in meters with the argument theRange (here set at 1000 km), and the 
# partition selection method can be assigned as either "checkerboard" or "random"
# In addition, the spatialBlock function returns a map showing the different spatial partitions
sb <- spatialBlock(speciesData = occsBg.sf, rasterLayer = envs.bg, theRange = 1000000, k = 5, selection = "random")
# Here, we can pull out the partition information from the SpatialBlock object to assemble
# occs.grp and bg.grp, which can be used for plotting or as user.grp inputs for ENMevaluate
occs.grp <- sb$foldID[1:nrow(occs)]
bg.grp <- sb$foldID[(nrow(occs)+1):length(sb$foldID)]
evalplot.grps(pts = bg, pts.grp = bg.grp, envs = envs.bg)
```

Choosing among these data partitioning methods depends on the research objectives and the characteristics of the study system. Refer to the [Resources](#resources) section for additional considerations on appropriate partitioning for evaluation.

## Running ENMeval {#eval}
Once you decide which method of data partitioning you would like to use, you are ready to start building models. We now move on to the main function in ENMeval: `ENMevaluate`.

- [Initial considerations](#eval.consid)
- [Different parameterizations](#eval.parameterizations)
- [Exploring the results (the ENMevaluate object)](#eval.explore)

#### Initial considerations {#eval.consid}
The two main parameters to define when calling `ENMevaluate` are (1) the range of regularization multiplier values and (2) the combinations of feature class to consider. The ***regularization multiplier*** (RM) determines the penalty for using variables or their transformations in the model. Higher RM values impose a stronger penalty on model complexity and thus result in simpler (*flatter*) model predictions. The ***feature classes*** determine the potential shape of the marginal response curves. A model that is only allowed to include linear feature classes will most likely be simpler than a model that is allowed to include all possible feature classes. A more detailed description of these parameters is available in the [Resources](#resources) section. For the purposes of this vignette, we demonstrate simply how to adjust these parameters. The following section deals with comparing the outputs of each model.

Unless you supply the function with background points (which is recommended in many cases), you will need to define how many background points should be used with the 'n.bg' argument. If any of your predictor variables are categorical (e.g., biomes), you will either need to define which layer(s) these are using the 'categoricals' argument or assign them as factors in the RasterStack (like we did here) or input table.

ENMevaluate builds a separate model for each unique combination of RM values and feature class combinations. For example, the following call will build and evaluate 2 models. One with RM=1 and another with RM=2, both allowing only linear features.


```{r enmeval1}
e.mx.l <- ENMevaluate(occs = occs, envs = envs, bg = bg, algorithm = 'maxnet', partitions = 'block', 
                      tune.args = list(fc = "L", rm = 1:2))
```

We may, however, want to compare a wider range of models that can use a wider variety of feature classes and regularization multipliers:

```{r maxnet2}
e.mx.lqh <- ENMevaluate(occs = occs, envs = envs, bg = bg, algorithm = 'maxnet', partitions = 'block', 
                        tune.args = list(fc = c("L","LQ","LQH","H"), rm = 1:3))
```

When building many models, the command may take a long time to run. Of course this depends on the size of your dataset and the computer you are using. When working on big projects, running the command in parallel (`parallel=TRUE`) can be considerably faster. Note that running parallel can also be slower when working on small datasets.

We can also calculate one of two niche overlap statistics while running `ENMevaluate` by setting the arguments `overlap=TRUE` and `overlapStat`, which support Moran's I or Schoener's D (see [Warren et al. (2008)](https://dx.doi.org/10.1111/j.1558-5646.2008.00482.x)). Note that you can also calculate this value at a later stage using the separate `calc.niche.overlap()` function.

```{r maxnet2.overlap, results='hide'}
overlap <- calc.niche.overlap(e.mx.lqh@predictions, overlapStat = "D")
overlap
```

#### Different parameterizations {#eval.parameterizations}
There are multiple ways to run the function `ENMevaluate()`, and we will go over how to specify each parameterization and what the effects are on the results.

```{r evalExamples}
# This example is for Maxent models, and so we will specify the tune.tbl with ranges of feature classes and regularization multipliers
tune.args <- list(fc = c("L", "LQ", "H", "LQH"), rm = 1:5)

# 1. Standard: we are using the R package maxnet to avoid frequent user issues with rJava
e.mx <- ENMevaluate(occs, envs, bg, algorithm = "maxnet", tune.args = tune.args, partitions = "block")
# If maxent.jar is installed and rJava loads properly, you can also run Maxent with the original Java software
e.mxjar <- ENMevaluate(occs, envs, bg, algorithm = "maxent.jar", tune.args = tune.args, partitions = "block")

# 2. Testing partition: no cross validation statistics calculated; instead, model will be evaluated on a testing dataset that is not used to create the full model
e.test <- ENMevaluate(occs, envs, bg, algorithm = "maxnet", tune.args = tune.args, partitions = "testing", occs.testing = occs.testing)

# 3. User partitions
user.grp <- list(occs.grp = round(runif(nrow(occs), 1, 2)), bg.grp = round(runif(nrow(bg), 1, 2)))
e.user <- ENMevaluate(occs, envs, bg, algorithm = "maxnet", tune.args = tune.args, partitions = "user", user.grp = user.grp)

# 4. No partitions: no cross validation statistics calculated, nor any model evaluation on test data
e.noCV <- ENMevaluate(occs, envs, bg, algorithm = "maxnet", tune.args = tune.args, partitions = "none")

# 5. No raster data (a.k.a, samples with data, or SWD): no full model raster predictions created, so will run faster; also, both cbi.train and cbi.val will be calculated on the point data (training and validation background) instead of on the "envs" rasters (default)
# For this implementation, assigning the categorical variable to factor with the argument "categoricals" is easier,
# as ENMevaluate() internally assigns the levels based on both occs and bg, avoiding any errors associated with
# different factor levels when combining data
occs.z <- cbind(occs, raster::extract(envs, occs))
bg.z <- cbind(bg, raster::extract(envs, bg))
e.swd <- ENMevaluate(occs.z, bg = bg.z, algorithm = "maxnet", tune.args = tune.args, partitions = "block")
```

You can also specify your own custom validation statistics for `ENMevaluate()` to calculate and report in the results tables. This is done by defining a custom function that has the arguments shown below, which are used by the ENMeval internal function `tune.validate()`. Here, we add functionality to calculate the AUC ratio and associated p-value for the partial ROC perfomance metric, defined in [Peterson et al. (2008)](http://dx.doi.org/10.1016/j.ecolmodel.2007.11.008) and implemented by the package [kuenm](http://dx.doi.org/10.7717/peerj.6281).

```{r}
# Define a custom function that implements a performance metric not included in ENMeval
# The function should have a single argument "vars", which is a list that includes the data
# most performance metrics should require -- the total list of these data can be found here: ?user.eval
# Make sure you return a data frame that specifies the names you want to see in the results tables
devtools::install_github("marlonecobos/kuenm")
library(kuenm)
proc <- function(vars) {
  proc <- kuenm::kuenm_proc(vars$occs.val.pred, c(vars$bg.train.pred, vars$bg.val.pred))
  out <- data.frame(proc_auc_ratio = proc$pROC_summary[1], proc_pval = proc$pROC_summary[2], row.names = NULL)
  return(out)
}

# Now we can run ENMevaluate() with the argument "user.eval", and simply give it the custom function
e.mx.proc <- ENMevaluate(occs, envs, bg, algorithm = "maxnet", tune.args = list(fc = "L", rm = 1:2), partitions = "block", user.eval = proc)
# We can see the new performance statistic averages in the results and results.partitions tables
e.mx.proc@results
e.mx.proc@results.partitions
```

The last way to run `ENMevaluate()` is by specifying a new algorithm using the `ENMdetails` object. Built-in algorithms (maxent.jar, maxnet) are implemented as `ENMdetails` objects---they can be found in the /R folder of the package with the file name "enm.name", where "name" is the algorithm. The `ENMdetails` object specifies the way `ENMevaluate()` should run the algorithm. It has some simple functions that output 1) the algorithm's name, 2) the function that runs the algorithm, 3) particular messages or errors, 4) the arguments for the model's function, 5) a specific parameterization of the `predict()` function, 6) the number of non-zero model coefficients, and 7) the variable importance table (if one is available from the model object). Users can also construct their own `ENMdetails` object using the built-in ones as guides. For example, a user can copy the "enm.maxnet.R" script, modify the code to specify a different model, save it as a new script in the /R folder with the name "enm.myAlgorithm", and use it to run `ENMevaluate()`. We plan to work with other research groups to identify best practices for tuning other algorithms with presence-background/pseudoabsence data and implement new algorithms in ENMeval in the future.


#### Exploring the results {#eval.explore}
Now let's take a look at the output from `ENMevaluate` (which is an object of class `ENMevaluation`) in more detail (also see `?ENMevaluation`).  It contains the following slots:

- `algorithm` A character vector showing which algorithm was used
- `tune.settings` A data.frame of settings that were tuned
- `partition.method` A character of partitioning method used
- `results` A data.frame of evaluation summary statistics
- `results.partitions` A data.frame of evaluation k-fold statistics
- `models` A list of model objects
- `predictions` A RasterStack of model predictions
- `occ.pts` A data.frame of occurrence coordinates used for model training
- `occs.grp` A vector of partition groups for occurrence points
- `bg.pts` A data.frame of background coordinates used for model training
- `bg.grp` A vector of partition groups for background points
- `overlap` A list of matrices of pairwise niche overlap statistics

Let's first examine the structure of the object:
```{r results1}
e.mx
# simplify the summary to look at higher level items
str(e.mx, max.level=2)
```

We can use helper functions to access the slots in the ENMevaluate object.
```{r algorithm}
# Access algorithm, tuning settings, and partition method information
eval.algorithm(e.mx)
eval.tune.settings(e.mx)
eval.partition.method(e.mx)
# Results table with summary statistics for cross validation on test data
eval.results(e.mx)
# Results table with cross validation statistics for each test partition
eval.results.partitions(e.mx)
# List of models with names corresponding to tune.args column label
eval.models(e.mx)
# The "betas" slot in a maxnet model is a named vector of the variable coefficients 
# and what kind they are (in R formula notation)
# Note that the html file that is created when maxent.jar is run is **not** kept
m1.mx <- eval.models(e.mx)[["fc.LQH_rm.1"]]
m1.mx$betas
# the enframe function from the tibble package makes this named vector into a more readable table
tibble::enframe(m1.mx$betas)
# For maxent.jar models, we can access this information in the lambdas slot
# The notation used here is difficult to decipher, so check out the [`rmaxent`](https://github.com/johnbaums/rmaxent/blob/master/) package
# available on Github for the `parse_lambdas()` function that makes it easier to read
m1.mxjar <- eval.models(e.mxjar)[["fc.LQH_rm.1"]]
m1.mxjar@lambdas
rmaxent::parse_lambdas(m1.mxjar)
# We can also get a long list of results statistics from the results slot
m1.mxjar@results
# RasterStack of model predictions (for extent of "envs") with names corresponding 
# to tune.args column label
eval.predictions(e.mx)
# Original occurrence data coordinates with associated predictor variable values
eval.occs(e.mx)
# Background data coordinates with associated predictor variable values
eval.bg(e.mx)
# Partition group assignments for occurrence data
eval.occs.grp(e.mx)
# Partition group assignments for background data
eval.bg.grp(e.mx)
```

## Plotting tuning results {#plot}
Plotting options in R are extremely flexible and here we demonstrate some key tools to explore the results of an ENMevaluate object graphically.

- [Plotting model predictions](#plot.preds)
- [Plotting marginal response curves](#plot.resp)

ENMeval has a built-in ggplot-based plotting function (`eval.plot`) to visualize the results of the different models you tuned. Here, we will plot AICc and delta.AICc values for each feature class for both regularization multipliers.

```{r plot.res}
evalplot.stats(e = e.mx, stats = "or.mtp", color = "fc", x.var = "rm")
# We can plot more than one statistic at one with ggplot facetting
evalplot.stats(e = e.mx, stats = c("or.mtp", "auc.val"), color = "fc", x.var = "rm")
# Sometimes the error bars make it hard to visualize the plot, so we can try turning them off
evalplot.stats(e = e.mx, stats = c("or.mtp", "auc.val"), color = "fc", x.var = "rm", error.bars = FALSE)
# We can also fiddle with the dodge argument to jitter the positions of overlapping points
evalplot.stats(e = e.mx, stats = c("or.mtp", "auc.val"), color = "fc", x.var = "rm", dodge = 0.5)
# Finally, we can switch which variables are on the x-axis and which symbolized by color
# ENMeval currently only accepts two variables for plotting at a time
evalplot.stats(e = e.mx, stats = c("or.mtp", "auc.val"), color = "rm", x.var = "fc", error.bars = FALSE)
```

#### Model selection {#eval.select}

Once we have our results, we will want to select one or more models that we think are optimal across all the models we ran. For this example, we will demonstrate how to select models using AICc (which does not consider cross-validation results) and a sequential method that selects models with the lowest average test omission rate, and to break ties, next with the highest average validation AUC (this method uses cross-validation results).

```{r evaluations}
# Overall results
res <- eval.results(e.mx)
opt.aicc <- res %>% filter(delta.AICc == 0)
opt.aicc
opt.seq <- res %>% 
  filter(or.10p.avg == min(or.10p.avg)) %>% 
  filter(auc.val.avg == max(auc.val.avg))
opt.seq
```

Let's now pull the optimal model (using the sequential criteria) from the model list and examine it.
```{r mod.obj1}
mod.seq <- eval.models(e.mx)[[opt.seq$tune.args]]
# The non-zero coefficients in our model
mod.seq$betas
# The marginal response curves for our model
plot(mod.seq, type = "cloglog")
dev.off()
# maxent.jar models use the dismo::response() function for this
dismo::response(eval.models(e.mxjar)[["fc.LQ_rm.5"]])
```

Now let's pull out and plot the prediction raster for our optimal model. Note that by default for maxent.jar or maxnet models, these predictions are in the 'cloglog' output format that is bounded between 0 and 1 (see Phillips et al. 2017 for more details). This can be changed with the pred.type argument in `ENMevaluate()`.
These predictions are for the entire extent of the input predictor variable rasters, and thus include areas outside of the background extent used for model training. Thus, we should interpret areas far outside this extent with caution.

```{r plotPred}
pred.seq <- eval.predictions(e.mx)[[opt.seq$tune.args]]
plot(pred.seq)
# Let's also plot the binned background points with the occurrence points on top
points(eval.bg(e.mx), pch = 3, col = eval.bg.grp(e.mx), cex = 0.5)
points(eval.occs(e.mx), pch = 21, bg = eval.occs.grp(e.mx))
```

Let's see how model complexity changes the predictions in our example. We'll compare the model built with only linear feature classes and the highest regularization multiplier value we used (i.e., fc='L', RM=5) with the model built with linear, quadratic, and hinge feature classes and the lowest regularization multiplier value we used (i.e., fc='LQH',  RM=1). We will first examine the marginal response curves, and then the mapped model model predictions. Notice how the simpler models tend to have more smooth predictions of suitability, while the complex ones tend to show more patchiness. For more on simplicity and complexity in ENMs, see [Merow et al. 2014](https://doi.org/10.1111/ecog.00845).

```{r plot.pred2}
# First, let's examine the non-zero model coefficients in the betas slot
# The simpler model has fewer model coefficients
mod.simple <- eval.models(e.mx)[['fc.L_rm.5']]
mod.complex <- eval.models(e.mx)[['fc.LQH_rm.1']]
mod.simple$betas
length(mod.simple$betas)
mod.complex$betas
length(mod.complex$betas)
# Next, let's take a look at the marginal response curves
# The complex model has marginal responses with more curves (quadratic terms) and spikes (hinge terms)
plot(mod.simple, type = "cloglog")
plot(mod.complex, type = "cloglog")
# Finally, let's cut the plotting area into two rows to visualize the predictions side-by-side
par(mfrow=c(2,1), mar=c(2,1,2,0))
# The simplest model: linear features only and high regularization
plot(eval.predictions(e.mx)[['fc.L_rm.5']], ylim=c(-30,20), xlim=c(-90,-30), legend=F, main='L_5 prediction')
# The most complex model: linear, quadratic, and hinge features with low regularization
plot(eval.predictions(e.mx)[['fc.LQH_rm.1']], ylim=c(-30,20), xlim=c(-90,-30), legend=F, main='LQH_1 prediction')
```

## Null models

```{r}
# We first run the null simulations with 100 iterations to get a reasonable null distribution for comparisons with the empirical values
mod.null <- ENMnulls(e.mx, mod.settings = list(fc = "LQ", rm = 5), no.iter = 100)
# We can inspect the results of each null simulation
null.results(mod.null)
# And even inspect the results of each partition of each null simulation
null.results.partitions(mod.null)
# For a summary, we can look at a comparison between the empirical and simulated results
emp.vs.null.results(mod.null)
# Finally, we can make plots of the null model results as a histogram
evalplot.nulls(mod.null, stats = c("or.10p", "auc.val"), plot.type = "histogram")
# Or a violin plot
evalplot.nulls(mod.null, stats = c("or.10p", "auc.val"), plot.type = "violin")
```

#### Metadata {#metadata}
ENMeval now catalogs details of the analysis essential for reproducibility in a rangeModelMetadata object, accessible from the output ENMevaluation object. The framework and philosophy behind rangeModelMetadata is described in [Merow et al. 2019](https://doi.org/10.1111/geb.12993). 

```{r}
# fill in a RMM object based on the ENMevaluate object
rmm <- eval.rmm(e.mx)
# Let's fill in our selection rules based on the sequential criteria we chose
rmm$model$selectionRules <- "lowest 10 percentile omission rate, break ties with average validation AUC"
# And enter our optimal model settings
rmm$model$finalModelSettings <- "LQ4"
# And then enter the details of our optimal model's prediction
rmm$prediction$continuous$minVal <- cellStats(pred.seq, min)
rmm$prediction$continuous$maxVal <- cellStats(pred.seq, max)
rmm$prediction$continuous$units <- "suitability (cloglog transformation)"
# This is just an example -- there may be more fields that you want to fill in
# Finally, save the metadata to CSV
rangeModelMetadata::rmmToCSV(rmm, "rmm_mx1.csv")
```


## Resources (*under construction*) {#resources}

###### Web Resources
- [Hijmans, R. and Elith, J. (2016) Species distribution modeling with R. dismo vignette.](https://cran.r-project.org/package=dismo)

- [Phillips, S. J. (2006) Phillips, S. (2006) A brief tutorial on Maxent. AT&T Research. Available at: https://biodiversityinformatics.amnh.org/open_source/maxent/](https://biodiversityinformatics.amnh.org/open_source/maxent/)

- [Yoder, J. (2013) Species distribution models in R. The Molecular Ecologist.](https://www.molecularecologist.com/2013/04/species-distribution-models-in-r/)

- [Maxent Google Group](https://groups.google.com/forum/embed/#!forum/maxent)

###### General Guides
- [Merow, C., Smith, M., and Silander, J.A. (2013) A practical guide to Maxent: what it does, and why inputs and settings matter. Ecography 36, 1-12.](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1600-0587.2013.07872.x)

- [Peterson, A.T., Soberón, J., Pearson, R.G., Anderson, R.P., Martínez-Meyer, E., Nakamura, M., and Araújo, M.B. (2011) Ecological Niches and Geographic Distributions. Monographs in Population Biology, 49. Princeton University Press.](https://press.princeton.edu/titles/9641.html)

- [Renner, I.W., Elith, J., Baddeley, A., Fithian, W., Hastie, T., Phillips, S.J., . . . Warton, D.I. (2015) Point process models for presence-only analysis. Methods in Ecology and Evolution 6, 366-379.](https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12352)

###### Environmental Similarity
= [Elith, J., Kearney, M., & Phillips, S. (2010). The art of modelling range‐shifting species. Methods in ecology and evolution, 1(4), 330-342.](https://doi.org/10.1111/j.2041-210X.2010.00036.x)

- [Owens, H. L., Campbell, L. P., Dornak, L. L., Saupe, E. E., Barve, N., Soberón, J., ... & Peterson, A. T. (2013). Constraints on interpretation of ecological niche models by limited environmental ranges on calibration areas. Ecological Modelling, 263, 10-18.](https://doi.org/10.1016/j.ecolmodel.2013.04.011)

###### Model Evaluation
- [Aiello-Lammens, M.E., Boria, R.A., Radosavljevic, A., Vilela, B., and Anderson, R.P. (2015) spThin: an R package for spatial thinning of species occurrence records for use in ecological niche models. Ecography 38, 541-545.](https://onlinelibrary.wiley.com/doi/abs/10.1111/ecog.01132)

- [Fielding, A.H. and Bell, J.F. (1997) A review of methods for the assessment of prediction errors in conservation presence-absence models. Environmental Conservation 24, 38-49.](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.463.359&rep=rep1&type=pdf)

- [Hijmans, R.J. (2012) Cross-validation of species distribution models: removing spatial sorting bias and calibration with a null model. Ecology 93, 679-688.](https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/11-0826.1)

- [Muscarella, R., Galante, P. J., Soley-Guardia, M., Boria, R. A., Kass, J. M., Uriarte, M. and Anderson, R. P. (2014), ENMeval: An R package for conducting spatially independent evaluations and estimating optimal model complexity for Maxent ecological niche models. Methods Ecol Evol, 5: 1198–1205.](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12261)

- [Radosavljevic, A. and Anderson, R.P. (2014) Making better Maxent models of species distributions: complexity, overfitting and evaluation. Journal of Biogeography 41, 629-643.](https://onlinelibrary.wiley.com/doi/abs/10.1111/jbi.12227)

- [Shcheglovitova, M. and Anderson, R.P. (2013) Estimating optimal complexity for ecological niche models: A jackknife approach for species with small sample sizes. Ecol. Model. 269, 9-17.](https://www.sciencedirect.com/science/article/pii/S0304380013004043)

- [Veloz, S.D. (2009) Spatially autocorrelated sampling falsely inflates measures of accuracy for presence-only niche models. Journal of Biogeography 36, 2290-2299.](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2699.2009.02174.x)

- [Wenger, S.J. and Olden, J.D. (2012) Assessing transferability of ecological models: an underappreciated aspect of statistical validation. Methods in Ecology and Evolution 3, 260-267.](https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/j.2041-210X.2011.00170.x)

###### Some Empirical Examples
- [Pearson, R.G., Raxworthy, C.J., Nakamura, M., and Peterson, A.T. (2007) Predicting species distributions from small numbers of occurrence records: a test case using cryptic geckos in Madagascar. Journal of Biogeography 34, 102-117.](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2699.2006.01594.x)

<!-- We may want to restrict the study extent to the continental areas (for example). We can use the simplified world map from the [`maptools`](https://cran.r-project.org/package=maptools) package and geoprocessing tools from the [`sf`](https://cran.r-project.org/package=sf) package to exclude the Caribbean from our study extent. -->

<!-- ```{r removeCaribbean, message=FALSE, warning = FALSE} -->
<!-- library(maptools) -->

<!-- # Get a simple world countries polygon -->
<!-- data(wrld_simpl) -->

<!-- # Get polygons for Central and South America -->
<!-- central.amer <- wrld_simpl@data$SUBREGION==5 -->
<!-- south.amer <- wrld_simpl@data$SUBREGION==13 -->
<!-- # Subset wrld_simpl and convert to sf -->
<!-- ca.sa <- st_as_sf(wrld_simpl[central.amer | south.amer,]) -->

<!-- # Buffer by 1 degree before masking to make sure you don't clip the coastline -->
<!-- ca.sa.buf <- st_buffer(ca.sa, dist = 1) -->

<!-- # Mask envs by the combined Central-South America polygon excluding the Caribbean -->
<!-- envs.bg <- mask(envs.bg, ca.sa.buf) -->

<!-- # Plot one of the new masked rasters -- we still have a few cells from the Galapagos, but are now missing the Caribbean -->
<!-- plot(envs.bg[[1]], main=names(envs.bg)[1]) -->
<!-- points(occs) -->
<!-- ``` -->
